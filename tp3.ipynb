{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b025f03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e63231",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On charge notre jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612a3a0e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://joon-kwon.github.io/optim-mathsv/blogData_train.csv', header=None)\n",
    "X = torch.from_numpy(data.values[:, :-1])\n",
    "y = torch.from_numpy(data.values[:, -1].reshape(-1))\n",
    "d = len(X[0])\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e31d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On définit notre fonction objectif qui correspond à une régression linéaire aux moindres carrés. On peut éventuellement passer en second argument un mini-batch (sous la forme d'une liste contenant les indices correspondants), si on souhaite ne calculer la valeur que sur ce mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a7e7d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x, minibatch=None):\n",
    "    if minibatch is None:\n",
    "        return torch.mean((torch.tensordot(X, x, dims=([1], [0])) - y)**2)\n",
    "    else:\n",
    "        return torch.mean((torch.tensordot(X[minibatch], x, dims=([1], [0])) - y[minibatch])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51389928",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "La différentiation automatique sous PyTorch fonctionne de la façon suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a0f3a81",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.4303e+03, -2.6301e+03, -3.2311e+01, -1.6469e+04, -2.9147e+03,\n",
      "        -1.4161e+03, -1.6398e+03, -3.8449e-01, -1.1305e+04, -9.8679e+02,\n",
      "        -1.2496e+03, -1.6925e+03,  0.0000e+00, -1.1273e+04, -5.8779e+02,\n",
      "        -2.8543e+03, -2.0955e+03, -3.2311e+01, -1.2334e+04, -2.5479e+03,\n",
      "        -1.6657e+02, -2.4195e+03,  8.7663e+03, -1.1119e+04,  7.3602e+00,\n",
      "        -3.0588e+01, -2.9762e+01, -2.2749e-02, -2.0534e+02, -2.5787e+01,\n",
      "        -1.1696e+01, -2.0189e+01,  0.0000e+00, -1.5924e+02, -5.0265e+00,\n",
      "        -1.1143e+01, -2.0311e+01,  0.0000e+00, -1.5923e+02, -8.3974e-03,\n",
      "        -2.8544e+01, -2.8059e+01, -2.2749e-02, -1.8192e+02, -1.9055e+01,\n",
      "        -5.5318e-01, -3.0101e+01,  1.3194e+02, -1.5385e+02, -7.3859e-03,\n",
      "        -3.1678e+03, -2.2541e+03, -6.9056e+02, -2.7200e+03, -1.5636e+03,\n",
      "        -2.9403e+01, -1.9855e+01, -6.8439e+00, -2.8239e+01, -1.3012e+01,\n",
      "        -2.3238e+02, -5.2429e+04, -3.5727e-02, -3.3457e+00,  0.0000e+00,\n",
      "        -3.7788e-03, -7.0982e+00, -2.7347e+00, -5.2424e+00, -3.8323e-02,\n",
      "        -2.0917e-02, -9.0753e-01,  0.0000e+00, -9.6570e-03, -9.1646e-02,\n",
      "         0.0000e+00, -5.3919e-01, -9.0463e-03, -4.4299e+00,  0.0000e+00,\n",
      "         0.0000e+00, -5.6225e-02, -2.5688e-02, -5.3438e-04, -2.1184e-02,\n",
      "        -2.7738e-01, -2.2902e-04, -2.8712e-01, -1.0278e+00, -4.2369e-03,\n",
      "        -3.8934e-03, -1.5497e-02, -6.1568e-02,  0.0000e+00, -5.7255e-04,\n",
      "        -1.8005e+00,  0.0000e+00, -1.6680e-02, -7.2142e-03, -2.3475e-02,\n",
      "        -1.0396e+01, -2.0530e+00, -5.3476e-01, -9.8590e-01, -6.7714e-02,\n",
      "        -1.4886e-02, -2.4772e-02, -2.5784e-01,  0.0000e+00, -4.3285e-01,\n",
      "        -5.3438e-04, -1.0688e-03, -5.3820e-03, -2.6953e+00, -1.3864e+00,\n",
      "        -4.2762e-01, -3.2903e-02, -2.2192e-01, -8.0192e-01, -3.2539e+00,\n",
      "        -5.4194e-01, -4.2114e+00, -3.0918e-03, -3.7025e-03, -4.8869e-01,\n",
      "        -1.4466e-02, -1.1203e-01, -3.5292e-01, -2.2902e-04,  0.0000e+00,\n",
      "        -4.7407e-02,  0.0000e+00, -1.3245e-02, -8.4280e-01, -5.7843e-01,\n",
      "        -9.8342e-01, -1.6528e-01, -6.2427e-01, -5.2162e+00, -1.3490e+00,\n",
      "        -1.4217e+00, -3.3666e-02, -1.0983e+01, -7.5233e-02, -1.9429e-02,\n",
      "        -1.7673e-02, -4.5957e-02, -4.9621e-04,  0.0000e+00, -4.8552e-02,\n",
      "        -5.0791e+00, -9.3135e-03, -7.5153e-01, -1.5125e+00, -3.8170e-05,\n",
      "         0.0000e+00, -9.8212e-02, -6.7981e+00, -2.3461e+00, -2.2864e-02,\n",
      "         0.0000e+00, -4.9419e-01, -5.5885e-01, -1.0722e-01, -1.2367e-02,\n",
      "         0.0000e+00, -1.0802e-02, -8.7791e-04,  0.0000e+00, -2.2967e+00,\n",
      "        -5.0957e-02,  0.0000e+00, -8.7791e-04, -3.2502e-01, -7.4241e-02,\n",
      "        -3.4239e-02, -4.6224e-02,  0.0000e+00,  0.0000e+00, -2.2426e+00,\n",
      "        -7.2783e-01, -1.3493e-01, -1.5218e+00, -2.5988e+00, -2.3665e-03,\n",
      "        -3.1112e-01, -3.6846e-01, -4.6835e-02, -2.6551e-01,  0.0000e+00,\n",
      "        -2.5352e+00, -2.7689e-01, -2.6566e+00, -3.5336e+00, -6.8309e-01,\n",
      "        -4.2167e-01, -1.5684e+00,  0.0000e+00, -5.2675e-03,  0.0000e+00,\n",
      "        -3.0231e-02, -3.2784e+00, -3.6262e-03, -1.1146e-02, -2.4383e-01,\n",
      "        -1.3390e-01, -1.2000e+00, -2.8902e-01, -3.6338e-02, -5.5344e+00,\n",
      "        -4.0766e-02,  0.0000e+00, -8.2023e+00, -1.4772e-02, -9.0349e-02,\n",
      "        -1.0886e+00, -7.8630e-03, -8.5348e-02, -8.7784e-01, -6.1454e-03,\n",
      "        -1.0769e+00, -1.8016e-02, -3.8170e-05, -2.6337e-03, -1.1440e-01,\n",
      "        -6.7281e+00, -2.1696e-01, -2.2522e+00, -1.3245e-02, -2.1732e+00,\n",
      "        -5.8744e-02, -7.6432e+00, -3.2597e+00, -8.2707e-01, -1.1184e-02,\n",
      "         0.0000e+00, -7.6226e-02, -3.2788e-02, -1.0840e-02, -2.3398e-02,\n",
      "        -7.0893e-01, -1.7093e-01,  0.0000e+00, -7.6340e-05, -3.1582e-01,\n",
      "        -1.0982e+01, -2.2047e+00, -4.6606e+00, -9.9471e-02,  0.0000e+00,\n",
      "        -1.5341e-01, -1.9043e-01, -1.1451e-04, -5.0824e-01, -2.7414e-01,\n",
      "        -1.8322e-03, -7.6787e-01, -3.6376e-02, -6.2408e-02, -5.5576e-02,\n",
      "        -2.4879e-01, -9.5425e-04, -1.4306e+00, -2.0003e+00, -2.1710e+00,\n",
      "        -2.2902e+00, -2.0051e+00, -1.9569e+00, -1.6752e+00, -2.1464e+00,\n",
      "        -2.4009e+00, -2.2303e+00, -1.9818e+00, -2.3093e+00, -1.3202e+00,\n",
      "        -1.1406e+00, -9.9834e-01,  0.0000e+00, -1.1906e+01, -8.3341e+00])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(d) # Le point en lequel on souhaite calculer un gradient\n",
    "x.requires_grad = True # On indique qu'on va calculer un gradient par rapport à cette variable\n",
    "value = f(x) # On calcule d'abord la fonction elle-même en ce point\n",
    "value.backward() # On demande à pytorch de calculer le gradient\n",
    "print(x.grad) # Le gradient correspondant est alors stocké dans x.grad\n",
    "x = x.detach() # On arrête la fonctionnalité de différentiation automatique (attention à sauver x.grad auparavant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479dcce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Question 1*: Utiliser la différentiation automatique pour implémenter une descente de gradient (à pas constant). Faire cesser les itérations au bout de 5 secondes (par exemple). Afficher la valeur de la fonction objectif en la dernière itérée. Essayer par tâtonnements différentes valeurs pour le pas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb90020",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On peut par exemple se donner un mini-batch de taille 100 de la façon suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c93f4c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17391, 20778, 20494, 14524, 46604, 44562, 50590, 18505, 38736, 22943, 33880, 34309, 41886, 29594, 46134, 1353, 37183, 30538, 40826, 26178, 24898, 33808, 33562, 48087, 693, 51388, 41985, 37972, 19327, 33833, 7831, 23374, 49318, 20205, 25006, 32624, 46555, 39514, 20944, 41166, 20836, 51194, 17192, 3086, 35423, 19537, 25987, 36673, 17060, 5648, 28480, 4279, 8687, 28170, 12191, 24009, 26727, 26653, 47064, 43525, 24524, 46250, 50564, 48769, 22311, 37564, 4835, 35131, 4547, 18556, 31830, 49446, 20200, 43030, 40357, 13850, 17392, 5302, 10533, 36936, 26593, 22285, 50059, 39057, 41078, 27185, 29046, 1578, 583, 42207, 31632, 50181, 22980, 12059, 27595, 1055, 52235, 25219, 17524, 30195]\n"
     ]
    }
   ],
   "source": [
    "minibatch = random.sample(range(0,n), 100)\n",
    "print(minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0fbed8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Question 2*: Modifier le code de la question 1 pour utiliser des mini-batchs. Essayer différentes tailles de mini-batchs. Commenter les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935ef12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PyTorch propose des implémentations d'algorithmes d'optimisation qui peuvent être utilisés de la façon suivante.\n",
    "`lr` correspond au pas. La prodécure `.step()` opère l'itération de l'algorithme d'optimisation en utilisant le gradient qui vient d'être calculé, et met à jour la valeur de la variable `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe80fc37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1377.3830, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(d)\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=1e-9)\n",
    "for t in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    value = f(x)\n",
    "    value.backward()\n",
    "    optimizer.step()\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f298e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Question 3*: À la place de SGD (qui correspond tout simplement à la descente de gradient), utiliser les implémentations de RMSprop, Adam, etc. (voir https://pytorch.org/docs/stable/optim.html), éventuellement avec des mini-batchs, toujours en stoppant l'exécution au bout de 5 secondes, et en affichant la valeur de la fonction objectif en la dernière itéreé. Comparer les résultats obtenus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
