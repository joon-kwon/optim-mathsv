{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c48094",
   "metadata": {},
   "source": [
    "Ce TD nécessite l'installation de la librairie PyTorch qui offre la fonctionnalité de différentiation automatique (et de faço optionnelle, le calcul sur GPU). Aucune manipulation n'est requise sur Google Colab, qui reste la solution la plus pratique. Si le TP est exécuté sur une machine locale avec la distribution Anaconda, il convient d'installer la librairie au préalable dans le console \"Anaconda Prompt\" à l'aide de la commande :\n",
    "```\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23fcb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657eef14",
   "metadata": {},
   "source": [
    "On charge notre jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e054e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://joon-kwon.github.io/optim-mathsv/blogData_train.csv', header=None)\n",
    "X = torch.from_numpy(data.values[:, :-1])\n",
    "y = torch.from_numpy(data.values[:, -1].reshape(-1))\n",
    "d = len(X[0])\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e77ec",
   "metadata": {},
   "source": [
    "On définit notre fonction objectif qui correspond à une régression linéaire aux moindres carrés. On peut éventuellement passer en second argument un mini-batch (sous la forme d'une liste contenant les indices correspondants), si on souhaite ne calculer la valeur que sur ce mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9896d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, minibatch=None):\n",
    "    if minibatch is None:\n",
    "        return torch.mean((torch.tensordot(X, x, dims=([1], [0])) - y)**2)\n",
    "    else:\n",
    "        return torch.mean((torch.tensordot(X[minibatch], x, dims=([1], [0])) - y[minibatch])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fd02e",
   "metadata": {},
   "source": [
    "La différentiation automatique sous PyTorch fonctionne de la façon suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47aaa486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.4303e+03, -2.6301e+03, -3.2311e+01, -1.6469e+04, -2.9147e+03,\n",
      "        -1.4161e+03, -1.6398e+03, -3.8449e-01, -1.1305e+04, -9.8679e+02,\n",
      "        -1.2496e+03, -1.6925e+03,  0.0000e+00, -1.1273e+04, -5.8779e+02,\n",
      "        -2.8543e+03, -2.0955e+03, -3.2311e+01, -1.2334e+04, -2.5479e+03,\n",
      "        -1.6657e+02, -2.4195e+03,  8.7663e+03, -1.1119e+04,  7.3602e+00,\n",
      "        -3.0588e+01, -2.9762e+01, -2.2749e-02, -2.0534e+02, -2.5787e+01,\n",
      "        -1.1696e+01, -2.0189e+01,  0.0000e+00, -1.5924e+02, -5.0265e+00,\n",
      "        -1.1143e+01, -2.0311e+01,  0.0000e+00, -1.5923e+02, -8.3974e-03,\n",
      "        -2.8544e+01, -2.8059e+01, -2.2749e-02, -1.8192e+02, -1.9055e+01,\n",
      "        -5.5318e-01, -3.0101e+01,  1.3194e+02, -1.5385e+02, -7.3859e-03,\n",
      "        -3.1678e+03, -2.2541e+03, -6.9056e+02, -2.7200e+03, -1.5636e+03,\n",
      "        -2.9403e+01, -1.9855e+01, -6.8439e+00, -2.8239e+01, -1.3012e+01,\n",
      "        -2.3238e+02, -5.2429e+04, -3.5727e-02, -3.3457e+00,  0.0000e+00,\n",
      "        -3.7788e-03, -7.0982e+00, -2.7347e+00, -5.2424e+00, -3.8323e-02,\n",
      "        -2.0917e-02, -9.0753e-01,  0.0000e+00, -9.6570e-03, -9.1646e-02,\n",
      "         0.0000e+00, -5.3919e-01, -9.0463e-03, -4.4299e+00,  0.0000e+00,\n",
      "         0.0000e+00, -5.6225e-02, -2.5688e-02, -5.3438e-04, -2.1184e-02,\n",
      "        -2.7738e-01, -2.2902e-04, -2.8712e-01, -1.0278e+00, -4.2369e-03,\n",
      "        -3.8934e-03, -1.5497e-02, -6.1568e-02,  0.0000e+00, -5.7255e-04,\n",
      "        -1.8005e+00,  0.0000e+00, -1.6680e-02, -7.2142e-03, -2.3475e-02,\n",
      "        -1.0396e+01, -2.0530e+00, -5.3476e-01, -9.8590e-01, -6.7714e-02,\n",
      "        -1.4886e-02, -2.4772e-02, -2.5784e-01,  0.0000e+00, -4.3285e-01,\n",
      "        -5.3438e-04, -1.0688e-03, -5.3820e-03, -2.6953e+00, -1.3864e+00,\n",
      "        -4.2762e-01, -3.2903e-02, -2.2192e-01, -8.0192e-01, -3.2539e+00,\n",
      "        -5.4194e-01, -4.2114e+00, -3.0918e-03, -3.7025e-03, -4.8869e-01,\n",
      "        -1.4466e-02, -1.1203e-01, -3.5292e-01, -2.2902e-04,  0.0000e+00,\n",
      "        -4.7407e-02,  0.0000e+00, -1.3245e-02, -8.4280e-01, -5.7843e-01,\n",
      "        -9.8342e-01, -1.6528e-01, -6.2427e-01, -5.2162e+00, -1.3490e+00,\n",
      "        -1.4217e+00, -3.3666e-02, -1.0983e+01, -7.5233e-02, -1.9429e-02,\n",
      "        -1.7673e-02, -4.5957e-02, -4.9621e-04,  0.0000e+00, -4.8552e-02,\n",
      "        -5.0791e+00, -9.3135e-03, -7.5153e-01, -1.5125e+00, -3.8170e-05,\n",
      "         0.0000e+00, -9.8212e-02, -6.7981e+00, -2.3461e+00, -2.2864e-02,\n",
      "         0.0000e+00, -4.9419e-01, -5.5885e-01, -1.0722e-01, -1.2367e-02,\n",
      "         0.0000e+00, -1.0802e-02, -8.7791e-04,  0.0000e+00, -2.2967e+00,\n",
      "        -5.0957e-02,  0.0000e+00, -8.7791e-04, -3.2502e-01, -7.4241e-02,\n",
      "        -3.4239e-02, -4.6224e-02,  0.0000e+00,  0.0000e+00, -2.2426e+00,\n",
      "        -7.2783e-01, -1.3493e-01, -1.5218e+00, -2.5988e+00, -2.3665e-03,\n",
      "        -3.1112e-01, -3.6846e-01, -4.6835e-02, -2.6551e-01,  0.0000e+00,\n",
      "        -2.5352e+00, -2.7689e-01, -2.6566e+00, -3.5336e+00, -6.8309e-01,\n",
      "        -4.2167e-01, -1.5684e+00,  0.0000e+00, -5.2675e-03,  0.0000e+00,\n",
      "        -3.0231e-02, -3.2784e+00, -3.6262e-03, -1.1146e-02, -2.4383e-01,\n",
      "        -1.3390e-01, -1.2000e+00, -2.8902e-01, -3.6338e-02, -5.5344e+00,\n",
      "        -4.0766e-02,  0.0000e+00, -8.2023e+00, -1.4772e-02, -9.0349e-02,\n",
      "        -1.0886e+00, -7.8630e-03, -8.5348e-02, -8.7784e-01, -6.1454e-03,\n",
      "        -1.0769e+00, -1.8016e-02, -3.8170e-05, -2.6337e-03, -1.1440e-01,\n",
      "        -6.7281e+00, -2.1696e-01, -2.2522e+00, -1.3245e-02, -2.1732e+00,\n",
      "        -5.8744e-02, -7.6432e+00, -3.2597e+00, -8.2707e-01, -1.1184e-02,\n",
      "         0.0000e+00, -7.6226e-02, -3.2788e-02, -1.0840e-02, -2.3398e-02,\n",
      "        -7.0893e-01, -1.7093e-01,  0.0000e+00, -7.6340e-05, -3.1582e-01,\n",
      "        -1.0982e+01, -2.2047e+00, -4.6606e+00, -9.9471e-02,  0.0000e+00,\n",
      "        -1.5341e-01, -1.9043e-01, -1.1451e-04, -5.0824e-01, -2.7414e-01,\n",
      "        -1.8322e-03, -7.6787e-01, -3.6376e-02, -6.2408e-02, -5.5576e-02,\n",
      "        -2.4879e-01, -9.5425e-04, -1.4306e+00, -2.0003e+00, -2.1710e+00,\n",
      "        -2.2902e+00, -2.0051e+00, -1.9569e+00, -1.6752e+00, -2.1464e+00,\n",
      "        -2.4009e+00, -2.2303e+00, -1.9818e+00, -2.3093e+00, -1.3202e+00,\n",
      "        -1.1406e+00, -9.9834e-01,  0.0000e+00, -1.1906e+01, -8.3341e+00])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(d) # Le point en lequel on souhaite calculer un gradient\n",
    "x.requires_grad = True # On indique qu'on va calculer un gradient par rapport à cette variable\n",
    "value = f(x) # On calcule d'abord la fonction elle-même en ce point\n",
    "value.backward() # On demande à pytorch de calculer le gradient\n",
    "print(x.grad) # Le gradient correspondant est alors stocké dans x.grad\n",
    "x = x.detach() # On arrête la fonctionnalité de différentiation automatique (attention à sauver x.grad auparavant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ac951",
   "metadata": {},
   "source": [
    "*Question 1*: Utiliser la différentiation automatique pour implémenter une descente de gradient (à pas constant). Faire cesser les itérations au bout de 5 secondes (par exemple). Afficher la valeur de la fonction objectif en la dernière itérée. Essayer par tâtonnements différentes valeurs pour le pas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3edaba",
   "metadata": {},
   "source": [
    "On peut par exemple se donner un mini-batch de taille 100 de la façon suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b51a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5637, 18997, 1809, 16409, 8448, 31281, 45607, 37121, 41448, 15953, 44146, 11833, 31044, 1068, 8178, 20351, 1919, 43602, 34816, 26528, 14434, 4838, 12740, 14288, 37090, 13234, 49911, 37147, 33516, 41755, 38279, 11431, 41757, 583, 43563, 48877, 33311, 10767, 45656, 17079, 18506, 39317, 5340, 10714, 36794, 38491, 20928, 30530, 51302, 27852, 7782, 23155, 1646, 15544, 3547, 10580, 41454, 25784, 46728, 17925, 17197, 3800, 47644, 18723, 38550, 27994, 2940, 39863, 40880, 21318, 2456, 3941, 28862, 8424, 47989, 50948, 46165, 11791, 46105, 38226, 38584, 14130, 38902, 49385, 21184, 44699, 4783, 51564, 38397, 13413, 40951, 32188, 3190, 23022, 52305, 12645, 6048, 10623, 15907, 43684]\n"
     ]
    }
   ],
   "source": [
    "minibatch = random.sample(range(0,n), 100)\n",
    "print(minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e82870",
   "metadata": {},
   "source": [
    "*Question 2*: Modifier le code de la question 1 pour utiliser des mini-batchs. Essayer différentes tailles de mini-batchs. Commenter les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277437a5",
   "metadata": {},
   "source": [
    "PyTorch propose des implémentations d'algorithmes d'optimisation qui peuvent être utilisés de la façon suivante.\n",
    "`lr` correspond au pas. La prodécure `.step()` opère l'itération de l'algorithme d'optimisation en utilisant le gradient qui vient d'être calculé, et met à jour la valeur de la variable `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8097eebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1377.3830, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(d)\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=1e-9)\n",
    "for t in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    value = f(x)\n",
    "    value.backward()\n",
    "    optimizer.step()\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef99e02",
   "metadata": {},
   "source": [
    "*Question 3*: À la place de SGD (qui correspond tout simplement à la descente de gradient), utiliser les implémentations de RMSprop, Adam, etc. (voir https://pytorch.org/docs/stable/optim.html), éventuellement avec des mini-batchs, toujours en stoppant l'exécution au bout de 5 secondes, et en affichant la valeur de la fonction objectif en la dernière itérée. Comparer les résultats obtenus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "name": "tp3.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
